{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to parallelize frame extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "from collections import defaultdict\n",
    "\n",
    "import face_recognition\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import pretorched.visualizers as vutils\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "# Plotting\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 18.0\n",
    "plt.rcParams['figure.figsize'] = (24.0, 16.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "# For MB Pro retina display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# For auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/data/datasets/DeepfakeDetection/'\n",
    "\n",
    "frame_dir = os.path.join(root, 'frames', 'dfdc_train_part_0', 'aaqaifqrwn.mp4')\n",
    "# frame_dir = os.path.join(root, 'frames', 'dfdc_train_part_0', 'gnlvnyrpfq.mp4')\n",
    "# frame_dir = os.path.join(root, 'frames', 'dfdc_train_part_2', 'cmliuimutv.mp4')\n",
    "test_image = os.path.join(frame_dir, '000200.jpg')\n",
    "# test_video = os.path.join(root, 'videos', 'dfdc_train_part_0','aaqaifqrwn.mp4')\n",
    "test_video = os.path.join(root, 'videos', 'dfdc_train_part_2','aljjmeqszq.mp4')\n",
    "# frame = face_recognition.load_image_file(test_image)\n",
    "# vutils.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_image = utils.extract_face_frame(test_image)\n",
    "# vutils.imshow(frame)\n",
    "vutils.imshow(face_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = utils.extract_multi_faces(test_video)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "f = np.stack([np.array(frame) for frame in faces])\n",
    "vutils.imshow(vutils.make_grid(f, nrow=16))\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = test_video\n",
    "v_margin=100\n",
    "h_margin=100\n",
    "batch_size=64\n",
    "fps=30\n",
    "device_id=0\n",
    "imsize=360\n",
    "num_frames = 16\n",
    "\n",
    "\n",
    "# Open video file\n",
    "video_capture = cv2.VideoCapture(video)\n",
    "video_capture.set(cv2.CAP_PROP_FPS, fps)\n",
    "faces = []\n",
    "frames = []\n",
    "frame_count = 0\n",
    "\n",
    "while video_capture.isOpened():\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Bail out when the video file ends\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    frame = frame[:, :, ::-1]\n",
    "\n",
    "    # Save each frame of the video to a list\n",
    "    frame_count += 1\n",
    "    frames.append(frame)\n",
    "#     if frame_count >= num_frames:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_face_locations = face_recognition.batch_face_locations(frames, number_of_times_to_upsample=0, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_face_location(frame, face_location, v_margin, h_margin, imsize):\n",
    "    top, right, bottom, left = face_location\n",
    "    mtop = max(top - v_margin, 0)\n",
    "    mbottom = min(bottom + v_margin, frame.shape[0])\n",
    "    mleft = max(left - h_margin, 0)\n",
    "    mright = min(right + h_margin, frame.shape[1])\n",
    "    face_image = frame[mtop:mbottom, mleft:mright]\n",
    "    return Image.fromarray(face_image).resize((imsize, imsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = [True, False]\n",
    "match.index(True)\n",
    "import pretorched.runners.utils as rutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match(known_faces, face_encoding, tolerance=0.50):\n",
    "    match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=tolerance)\n",
    "    try:\n",
    "        face_idx = match.index(True)\n",
    "        return face_idx\n",
    "    except ValueError:\n",
    "        return get_match(known_faces, face_encoding, tolerance + 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = defaultdict(list)\n",
    "known_faces = []\n",
    "\n",
    "for frameno, (frame, face_locations) in enumerate(zip(frames,batched_face_locations)):\n",
    "    number_of_faces_in_frame = len(face_locations)\n",
    "    print(frameno, number_of_faces_in_frame)\n",
    "    \n",
    "    if number_of_faces_in_frame < 1:\n",
    "        raise ValueError('WARNING TODO: NEED TO TRY ANOTHER METHOD')\n",
    "    elif number_of_faces_in_frame > 1:\n",
    "        print('MORE THAN ONE FACE')\n",
    "            \n",
    "        face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "        \n",
    "        if not known_faces:\n",
    "            known_faces = face_encodings\n",
    "#             known_faces = [rutils.AverageMeter(i) for i in range(len(face_encodings))]\n",
    "#             for m, fe in zip(known_faces, face_encodings):\n",
    "#                 m.update(fe)\n",
    "            \n",
    "        for face_encoding, face_location in zip(face_encodings, face_locations):\n",
    "            # See if the face is a match for the known face(s)\n",
    "#             match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.60)\n",
    "            face_idx = get_match(known_faces, face_encoding)\n",
    "#             face_idx = get_match([m.avg for m in known_faces], face_encoding)\n",
    "    \n",
    "            face_image = crop_face_location(frame, face_location, v_margin, h_margin, imsize)\n",
    "#             print(match)\n",
    "#             face_idx = match.index(True)\n",
    "            faces[face_idx].append(face_image)\n",
    "            known_faces[face_idx] = face_encoding\n",
    "#             known_faces[face_idx].update(face_encoding)\n",
    "    \n",
    "    else:\n",
    "        for i, face_location in enumerate(face_locations):\n",
    "            # Print the location of each face in this frame\n",
    "            face_image = crop_face_location(frame, face_location, v_margin, h_margin, imsize)\n",
    "            faces[i].append(face_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, images in faces.items():\n",
    "    f = np.stack([np.array(frame) for frame in images])\n",
    "    vutils.imshow(vutils.make_grid(f, nrow=16))\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(frames, imsize):\n",
    "    face_batch = []\n",
    "    batch_of_face_locations = face_recognition.batch_face_locations(frames, number_of_times_to_upsample=0)\n",
    "\n",
    "    for frame_number_in_batch, face_locations in enumerate(batch_of_face_locations):\n",
    "        number_of_faces_in_frame = len(face_locations)\n",
    "\n",
    "        for frame, face_location in zip(frames, face_locations):\n",
    "            # Print the location of each face in this frame\n",
    "            top, right, bottom, left = face_location\n",
    "            mtop = max(top - v_margin, 0)\n",
    "            mbottom = min(bottom + v_margin, frame.shape[0])\n",
    "            mleft = max(left - h_margin, 0)\n",
    "            mright = min(right + h_margin, frame.shape[1])\n",
    "            # face_image = frame[top - v_margin:bottom + v_margin, left - h_margin:right + h_margin]\n",
    "            face_image = frame[mtop:mbottom, mleft:mright]\n",
    "            face_batch.append(Image.fromarray(face_image).resize((imsize, imsize)))\n",
    "    return face_batch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
