{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import Parameter as P\n",
    "import pretorched\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Video\n",
    "from pretorched.visualizers import grad_cam, visualize_samples\n",
    "\n",
    "import models\n",
    "from data import VideoFolder\n",
    "\n",
    "# class Normalize(nn.Module):\n",
    "#     def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],\n",
    "#                  shape=(1, -1, 1, 1, 1), rescale=True):\n",
    "#         super().__init__()\n",
    "#         self.shape = shape\n",
    "#         self.mean = P(torch.tensor(mean).view(shape),\n",
    "#                       requires_grad=False)\n",
    "#         self.std = P(torch.tensor(std).view(shape),\n",
    "#                      requires_grad=False)\n",
    "#         self.rescale = rescale\n",
    "\n",
    "#     def forward(self, x, rescale=None):\n",
    "#         rescale = self.rescale if rescale is None else rescale\n",
    "#         x.div_(255.) if rescale else None\n",
    "#         return (x - self.mean) / self.std\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "WEIGHT_DIR = 'weights'\n",
    "video_dir = 'DeepfakeDetection/test_videos'\n",
    "cam_dir = 'cam_videos'\n",
    "os.makedirs(cam_dir, exist_ok=True)\n",
    "checkpoint_file = 'resnet18_dfdc_seg_count-24_init-imagenet-ortho_optim-Ranger_lr-0.001_sched-CosineAnnealingLR_bs-64_best.pth.tar'\n",
    "fakenet = pretorched.resnet18(num_classes=2, pretrained=None)\n",
    "fakenet.load_state_dict({k.replace('module.model.', ''): v\n",
    "                         for k, v in torch.load(os.path.join(WEIGHT_DIR, checkpoint_file))['state_dict'].items()})\n",
    "fakenet.eval()\n",
    "facenet = models.FaceModel(size=fakenet.input_size[-1],\n",
    "                               device=device,\n",
    "                               margin=100,\n",
    "                               min_face_size=50,\n",
    "                               keep_all=True,\n",
    "                               post_process=False,\n",
    "                               select_largest=False,\n",
    "                               chunk_size=150)\n",
    "fakenet = fakenet.to(device)\n",
    "gcam = grad_cam.GradCAM(model=fakenet)\n",
    "norm = models.Normalize().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoFolder(video_dir, step=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    filename, video, label = dataset[i]\n",
    "    video = video.unsqueeze(0)\n",
    "    video = video.to(device)\n",
    "    faces = facenet(video)\n",
    "    norm_faces = norm(faces)\n",
    "#     input_faces = norm_faces[0].transpose(0, 1)[0:1]\n",
    "    input_faces = norm_faces[0].transpose(0, 1)\n",
    "    print(input_faces.shape)\n",
    "    gcam.model.zero_grad()\n",
    "    probs, idx = gcam.forward(input_faces)\n",
    "    print(filename, label)\n",
    "#     print(gcam.preds.tolist(), torch.argmax(gcam.preds).item())\n",
    "    print(probs, idx)\n",
    "    \n",
    "    gcam.backward(idx=idx[0].unsqueeze(0).repeat(input_faces.size(0), 1))\n",
    "    output = gcam.generate(target_layer='layer4')\n",
    "    print(output.shape)\n",
    "    print(faces.shape)\n",
    "    raw_image = faces[0].permute(1, 2, 3, 0).detach().cpu().numpy() * 255\n",
    "    print(raw_image.shape)\n",
    "    output_video = [grad_cam.apply_heatmap(o, r).astype(np.uint8) for o, r in zip(output, raw_image)]\n",
    "#     output_image = grad_cam.apply_heatmap(output, raw_image).astype(np.uint8)\n",
    "    output_camfile = os.path.join(cam_dir, 'cam_' + filename)\n",
    "    pretorched.data.utils.array_to_video(output_video, output_camfile)\n",
    "#     for j in range(1, 10):\n",
    "#         plt.imshow(output_video[j])\n",
    "#         plt.show()\n",
    "#         plt.imshow(output[j])\n",
    "#         plt.show()\n",
    "#     plt.imshow(output_video[0])\n",
    "#     plt.show()\n",
    "#     plt.imshow(output_video[1])\n",
    "#     plt.show()\n",
    "#     plt.imshow(output_video[2])\n",
    "#     pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "num_frames = len(output_video)\n",
    "print(num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(frame=(0, num_frames))\n",
    "def show_frame(frame=1):\n",
    "    plt.imshow(output_video[frame])\n",
    "    plt.minorticks_off()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretorched.data.utils.array_to_video(output_video, 'test_video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video('test_video.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filename, video, label = dataset[1]\n",
    "video = video.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = video.to(device)\n",
    "faces = facenet(video)\n",
    "norm_faces = norm(faces)\n",
    "print(video.shape)\n",
    "print(faces.shape)\n",
    "print(norm_faces.shape)\n",
    "input_faces = norm_faces[0].transpose(0, 1)[0:1]\n",
    "print(input_faces.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, idx = gcam.forward(input_faces)\n",
    "\n",
    "print(f'probs: {probs.shape}')\n",
    "print(f'ids: {idx.shape}')\n",
    "\n",
    "gcam.backward(idx=idx[0])\n",
    "output = gcam.generate(target_layer='layer4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "raw_image = faces[0, :, 0].permute(1, 2, 0).detach().cpu().numpy() * 255\n",
    "h, w, _ = raw_image.shape\n",
    "output = cv2.resize(output, (w, h))\n",
    "output = cv2.applyColorMap(np.uint8(output * 255.0), cv2.COLORMAP_JET)\n",
    "output = 0.2 * output.astype(np.float) + 0.8 * raw_image.astype(np.float)\n",
    "output_image = output / output.max() * 255.0\n",
    "# output_image = grad_cam.apply_heatmap(output, raw_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((raw_image).astype(np.uint8))\n",
    "plt.imshow((output_image).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda0e43dda49bc244bab8a767061fdc276e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}